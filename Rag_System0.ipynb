{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPE48sD25pdvMRAD6Tgnh6U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khandekar0708/khandekar0708.github.io/blob/main/Rag_System0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai pinecone-client\n"
      ],
      "metadata": {
        "id": "KciamkVW5FIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Set your API keys from environment variables\n",
        "openai.api_key = \"OPENAI_API_KEY\"\n",
        "pinecone_api_key = \"PINECONE_API_KEY\"\n",
        "\n",
        "# Initialize Pinecone client\n",
        "pc = Pinecone(api_key=pinecone_api_key)\n",
        "\n",
        "# Define index name and specifications for free plan in us-east-1 (AWS)\n",
        "index_name = \"business-documents\"\n",
        "spec = ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")  # Free-tier compatible region\n",
        "\n",
        "# Create or connect to a Pinecone index\n",
        "try:\n",
        "    # Get the list of indexes\n",
        "    existing_indexes = pc.list_indexes()\n",
        "\n",
        "    # Check if the index already exists\n",
        "    if index_name not in existing_indexes:\n",
        "        pc.create_index(name=index_name, dimension=1536, metric=\"cosine\", spec=spec)\n",
        "\n",
        "    index = pc.Index(index_name)  # Connect to the index\n",
        "except Exception as e:\n",
        "    if \"already exists\" in str(e).lower():\n",
        "        print(f\"Index '{index_name}' already exists. Connecting to the existing index.\")\n",
        "        index = pc.Index(index_name)\n",
        "    else:\n",
        "        print(f\"Failed to create or connect to index: {e}\")\n",
        "        raise  # Re-raise if it's a different error\n",
        "\n",
        "# Function to get embeddings\n",
        "def get_embedding(text):\n",
        "    try:\n",
        "        response = openai.Embedding.create(input=[text], model=\"text-embedding-ada-002\")\n",
        "        return response['data'][0]['embedding']\n",
        "    except Exception as e:\n",
        "        print(\"Error in getting embedding:\", e)\n",
        "        return None\n",
        "\n",
        "# Ingest documents\n",
        "def ingest_documents(documents):\n",
        "    vectors = []\n",
        "    for doc in documents:\n",
        "        embedding = get_embedding(doc['text'])\n",
        "        if embedding is not None:  # Check if embedding was successful\n",
        "            vectors.append({\"id\": doc['id'], \"values\": embedding, \"metadata\": {\"text\": doc['text']}})\n",
        "    if vectors:  # Only upsert if there are vectors to add\n",
        "        index.upsert(vectors)\n",
        "\n",
        "# Function to search documents\n",
        "def search_documents(query, top_k=5):\n",
        "    query_embedding = get_embedding(query)\n",
        "    if query_embedding is None:\n",
        "        return []  # Early return if embedding failed\n",
        "    results = index.query(query_embedding, top_k=top_k, include_metadata=True)\n",
        "    return [result['metadata']['text'] for result in results['matches']]\n",
        "\n",
        "# RAG answer generation\n",
        "def rag_answer(query):\n",
        "    retrieved_docs = search_documents(query)\n",
        "    context = \"\\n\".join(retrieved_docs)\n",
        "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    try:\n",
        "        # Use the latest model available\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",  # Change to \"gpt-4\" if you have access\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=200,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        return response['choices'][0]['message']['content'].strip()\n",
        "    except Exception as e:\n",
        "        print(\"Error in generating answer:\", e)\n",
        "        return \"Sorry, I couldn't generate an answer.\"\n",
        "\n",
        "# Example usage\n",
        "documents = [\n",
        "    {\"id\": \"doc1\", \"text\": \"Product X has features A, B, and C, which are useful for business applications.\"},\n",
        "    {\"id\": \"doc2\", \"text\": \"Our company's support services are available 24/7 for all clients.\"},\n",
        "    {\"id\": \"doc3\", \"text\": \"The pricing plan includes Basic, Premium, and Enterprise tiers, each with different features.\"}\n",
        "]\n",
        "\n",
        "ingest_documents(documents)\n",
        "\n",
        "query = \"What support options does the company offer?\"\n",
        "answer = rag_answer(query)\n",
        "print(\"Answer:\", answer)\n"
      ],
      "metadata": {
        "id": "MjR887f_5F2z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}